{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d82e896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69fc078b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product name to search on Amazon.in: Guitar\n",
      "Data has been saved to amazon_products.csv\n"
     ]
    }
   ],
   "source": [
    "# Q.1 Answer:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_amazon_search_results(query, num_pages=3):\n",
    "    base_url = \"https://www.amazon.in/s\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    product_list = []\n",
    "\n",
    "    for page in range(1, num_pages + 1):\n",
    "        params = {\n",
    "            \"k\": query,\n",
    "            \"page\": page\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        products = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
    "\n",
    "        for product in products:\n",
    "            try:\n",
    "                brand_name = product.find(\"span\", {\"class\": \"a-size-base-plus\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                brand_name = \"-\"\n",
    "\n",
    "            try:\n",
    "                product_name = product.find(\"span\", {\"class\": \"a-size-medium\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                product_name = \"-\"\n",
    "\n",
    "            try:\n",
    "                price = product.find(\"span\", {\"class\": \"a-price-whole\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                price = \"-\"\n",
    "\n",
    "            try:\n",
    "                product_url = \"https://www.amazon.in\" + product.find(\"a\", {\"class\": \"a-link-normal\"})[\"href\"]\n",
    "            except AttributeError:\n",
    "                product_url = \"-\"\n",
    "\n",
    "           \n",
    "            product_response = requests.get(product_url, headers=headers)\n",
    "            product_soup = BeautifulSoup(product_response.content, \"html.parser\")\n",
    "\n",
    "            try:\n",
    "                return_exchange = product_soup.find(\"a\", {\"id\": \"RETURNS_POLICY\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                return_exchange = \"-\"\n",
    "\n",
    "            try:\n",
    "                expected_delivery = product_soup.find(\"span\", {\"id\": \"mir-layout-DELIVERY_BLOCK\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                expected_delivery = \"-\"\n",
    "\n",
    "            try:\n",
    "                availability = product_soup.find(\"span\", {\"class\": \"a-declarative\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                availability = \"-\"\n",
    "\n",
    "            product_list.append({\n",
    "                \"Brand Name\": brand_name,\n",
    "                \"Name of the Product\": product_name,\n",
    "                \"Price\": price,\n",
    "                \"Return/Exchange\": return_exchange,\n",
    "                \"Expected Delivery\": expected_delivery,\n",
    "                \"Availability\": availability,\n",
    "                \"Product URL\": product_url\n",
    "            })\n",
    "\n",
    "    return product_list\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    product_name = input(\"Enter the product name to search on Amazon.in: \")\n",
    "    results = get_amazon_search_results(product_name, num_pages=3)\n",
    "    save_to_csv(results, \"amazon_products.csv\")\n",
    "    print(\"Data has been saved to amazon_products.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf052c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.2. Answer:-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def get_amazon_search_results(query, num_pages=3):\n",
    "    base_url = \"https://www.amazon.in/s\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    product_list = []\n",
    "\n",
    "    for page in range(1, num_pages + 1):\n",
    "        params = {\n",
    "            \"k\": query,\n",
    "            \"page\": page\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        products = soup.find_all(\"div\", {\"data-component-type\": \"s-search-result\"})\n",
    "\n",
    "        for product in products:\n",
    "            try:\n",
    "                brand_name = product.find(\"span\", {\"class\": \"a-size-base-plus\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                brand_name = \"-\"\n",
    "\n",
    "            try:\n",
    "                product_name = product.find(\"span\", {\"class\": \"a-size-medium\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                product_name = \"-\"\n",
    "\n",
    "            try:\n",
    "                price = product.find(\"span\", {\"class\": \"a-price-whole\"}).text.strip()\n",
    "            except AttributeError:\n",
    "                price = \"-\"\n",
    "\n",
    "            try:\n",
    "                product_url = \"https://www.amazon.in\" + product.find(\"a\", {\"class\": \"a-link-normal\"})[\"href\"]\n",
    "            except AttributeError:\n",
    "                product_url = \"-\"\n",
    "\n",
    "            \n",
    "            if product_url != \"-\":\n",
    "                product_response = requests.get(product_url, headers=headers)\n",
    "                product_soup = BeautifulSoup(product_response.content, \"html.parser\")\n",
    "\n",
    "                try:\n",
    "                    return_exchange = product_soup.find(\"a\", {\"id\": \"RETURNS_POLICY\"}).text.strip()\n",
    "                except AttributeError:\n",
    "                    return_exchange = \"-\"\n",
    "\n",
    "                try:\n",
    "                    expected_delivery = product_soup.find(\"span\", {\"id\": \"mir-layout-DELIVERY_BLOCK\"}).text.strip()\n",
    "                except AttributeError:\n",
    "                    expected_delivery = \"-\"\n",
    "\n",
    "                try:\n",
    "                    availability = product_soup.find(\"span\", {\"class\": \"a-declarative\"}).text.strip()\n",
    "                except AttributeError:\n",
    "                    availability = \"-\"\n",
    "            else:\n",
    "                return_exchange = \"-\"\n",
    "                expected_delivery = \"-\"\n",
    "                availability = \"-\"\n",
    "\n",
    "            product_list.append({\n",
    "                \"Brand Name\": brand_name,\n",
    "                \"Name of the Product\": product_name,\n",
    "                \"Price\": price,\n",
    "                \"Return/Exchange\": return_exchange,\n",
    "                \"Expected Delivery\": expected_delivery,\n",
    "                \"Availability\": availability,\n",
    "                \"Product URL\": product_url\n",
    "            })\n",
    "\n",
    "           \n",
    "            time.sleep(1)\n",
    "\n",
    "    return product_list\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    product_name = input(\"Enter the product name to search on Amazon.in: \")\n",
    "    results = get_amazon_search_results(product_name, num_pages=3)\n",
    "    save_to_csv(results, \"amazon_products.csv\")\n",
    "    print(\"Data has been saved to amazon_products.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efac119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.3.Answers:-\n",
    "!pip install selenium\n",
    "!pip install beautifulsoup4\n",
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd16c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "\n",
    "\n",
    "if not os.path.exists('google_images'):\n",
    "    os.makedirs('google_images')\n",
    "\n",
    "\n",
    "def scrape_images(query):\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get('https://images.google.com')\n",
    "\n",
    "   \n",
    "    search_box = driver.find_element('name', 'q')\n",
    "    search_box.send_keys(query)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "    \n",
    "    for _ in range(5):\n",
    "        driver.execute_script(\"window.scrollBy(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  \n",
    "\n",
    "   \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    img_tags = soup.find_all('img', {'class': 'rg_i'}, limit=10)\n",
    "\n",
    "    img_urls = []\n",
    "    for img in img_tags:\n",
    "        try:\n",
    "            img_url = img['src']\n",
    "            if img_url:\n",
    "                img_urls.append(img_url)\n",
    "        except KeyError:\n",
    "            try:\n",
    "                img_url = img['data-src']\n",
    "                if img_url:\n",
    "                    img_urls.append(img_url)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "    \n",
    "    for i, img_url in enumerate(img_urls):\n",
    "        try:\n",
    "            img_data = requests.get(img_url).content\n",
    "            with open(f'google_images/{query}_{i+1}.jpg', 'wb') as handler:\n",
    "                handler.write(img_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not save image {i+1} for query '{query}': {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for keyword in keywords:\n",
    "        print(f\"Scraping images for: {keyword}\")\n",
    "        scrape_images(keyword)\n",
    "        print(f\"Finished scraping images for: {keyword}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc284c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.4.Answer:-\n",
    "!pip install selenium\n",
    "!pip install beautifulsoup4\n",
    "!pip install pandas\n",
    "!pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca8984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "def get_flipkart_search_results(query):\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get('https://www.flipkart.com')\n",
    "\n",
    "   \n",
    "    try:\n",
    "        close_button = driver.find_element('xpath', '/html/body/div[2]/div/div/button')\n",
    "        close_button.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    search_box = driver.find_element('name', 'q')\n",
    "    search_box.send_keys(query)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "    time.sleep(3)  # Allow time for the page to load\n",
    "\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    return soup\n",
    "\n",
    "def parse_flipkart_results(soup):\n",
    "    product_list = []\n",
    "\n",
    "    products = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "\n",
    "    for product in products:\n",
    "        details = {}\n",
    "\n",
    "        try:\n",
    "            details['Brand Name'] = product.find('div', {'class': '_4rR01T'}).text.split()[0]\n",
    "        except AttributeError:\n",
    "            details['Brand Name'] = \"-\"\n",
    "\n",
    "        try:\n",
    "            details['Smartphone name'] = product.find('div', {'class': '_4rR01T'}).text\n",
    "        except AttributeError:\n",
    "            details['Smartphone name'] = \"-\"\n",
    "\n",
    "        try:\n",
    "            details['Price'] = product.find('div', {'class': '_30jeq3 _1_WHN1'}).text\n",
    "        except AttributeError:\n",
    "            details['Price'] = \"-\"\n",
    "\n",
    "        try:\n",
    "            product_url = \"https://www.flipkart.com\" + product.find('a', {'class': '_1fQZEK'})['href']\n",
    "            details['Product URL'] = product_url\n",
    "        except (AttributeError, TypeError):\n",
    "            details['Product URL'] = \"-\"\n",
    "\n",
    "        \n",
    "        if details['Product URL'] != \"-\":\n",
    "            product_response = requests.get(details['Product URL'])\n",
    "            product_soup = BeautifulSoup(product_response.content, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                specifications = product_soup.find_all('li', {'class': '_21Ahn-'})\n",
    "                for spec in specifications:\n",
    "                    text = spec.text\n",
    "                    if \"RAM\" in text:\n",
    "                        details['RAM'] = text.split('|')[0].strip()\n",
    "                        details['Storage(ROM)'] = text.split('|')[1].strip()\n",
    "                    elif \"Display\" in text:\n",
    "                        details['Display Size'] = text.split(':')[1].strip()\n",
    "                    elif \"Battery\" in text:\n",
    "                        details['Battery Capacity'] = text.split(':')[1].strip()\n",
    "                    elif \"Primary Camera\" in text:\n",
    "                        details['Primary Camera'] = text.split(':')[1].strip()\n",
    "                    elif \"Secondary Camera\" in text:\n",
    "                        details['Secondary Camera'] = text.split(':')[1].strip()\n",
    "                    elif \"Color\" in text:\n",
    "                        details['Colour'] = text.split(':')[1].strip()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "       \n",
    "        details['RAM'] = details.get('RAM', '-')\n",
    "        details['Storage(ROM)'] = details.get('Storage(ROM)', '-')\n",
    "        details['Display Size'] = details.get('Display Size', '-')\n",
    "        details['Battery Capacity'] = details.get('Battery Capacity', '-')\n",
    "        details['Primary Camera'] = details.get('Primary Camera', '-')\n",
    "        details['Secondary Camera'] = details.get('Secondary Camera', '-')\n",
    "        details['Colour'] = details.get('Colour', '-')\n",
    "\n",
    "        product_list.append(details)\n",
    "\n",
    "    return product_list\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Enter the smartphone name to search on Flipkart: \")\n",
    "    soup = get_flipkart_search_results(query)\n",
    "    results = parse_flipkart_results(soup)\n",
    "    save_to_csv(results, \"flipkart_smartphones.csv\")\n",
    "    print(\"Data has been saved to flipkart_smartphones.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#.Q.5.Answer:-\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "def get_coordinates(city_name):\n",
    "    # Set up Selenium WebDriver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    driver.get('https://www.google.com/maps')\n",
    "\n",
    "    \n",
    "    search_box = driver.find_element('id', 'searchboxinput')\n",
    "    search_box.send_keys(city_name)\n",
    "    search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "   \n",
    "    time.sleep(5)\n",
    "\n",
    "    \n",
    "    current_url = driver.current_url\n",
    "\n",
    "    \n",
    "    try:\n",
    "        coordinates_part = current_url.split('@')[1].split(',')[:2]\n",
    "        latitude = coordinates_part[0]\n",
    "        longitude = coordinates_part[1]\n",
    "    except (IndexError, AttributeError):\n",
    "        latitude = \"-\"\n",
    "        longitude = \"-\"\n",
    "\n",
    "    \n",
    "    driver.quit()\n",
    "\n",
    "    return latitude, longitude\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city_name = input(\"Enter the city name to search on Google Maps: \")\n",
    "    latitude, longitude = get_coordinates(city_name)\n",
    "    print(f\"Coordinates of {city_name}:\\nLatitude: {latitude}\\nLongitude: {longitude}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8a1780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.6.Answer:-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_gaming_laptops(url):\n",
    "   \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "   \n",
    "    laptop_list = []\n",
    "    containers = soup.find_all('div', class_='product-card-new')\n",
    "    \n",
    "    for container in containers:\n",
    "        laptop_details = {}\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            laptop_details['Name'] = container.find('h3', class_='heading').text.strip()\n",
    "        except AttributeError:\n",
    "            laptop_details['Name'] = \"-\"\n",
    "        \n",
    "        try:\n",
    "            laptop_details['Specs'] = container.find('div', class_='Specs').text.strip()\n",
    "        except AttributeError:\n",
    "            laptop_details['Specs'] = \"-\"\n",
    "        \n",
    "        try:\n",
    "            laptop_details['Price'] = container.find('span', class_='price').text.strip()\n",
    "        except AttributeError:\n",
    "            laptop_details['Price'] = \"-\"\n",
    "        \n",
    "        try:\n",
    "            laptop_details['Rating'] = container.find('div', class_='rating').text.strip()\n",
    "        except AttributeError:\n",
    "            laptop_details['Rating'] = \"-\"\n",
    "        \n",
    "        try:\n",
    "            laptop_details['URL'] = container.find('a', class_='primary-link')['href']\n",
    "        except AttributeError:\n",
    "            laptop_details['URL'] = \"-\"\n",
    "        \n",
    "        laptop_list.append(laptop_details)\n",
    "    \n",
    "    return laptop_list\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.digit.in/top-products/best-gaming-laptops-40.html'  \n",
    "    gaming_laptops = get_gaming_laptops(url)\n",
    "    save_to_csv(gaming_laptops, \"best_gaming_laptops.csv\")\n",
    "    print(\"Data has been saved to best_gaming_laptops.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2dcb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.7. Answer:-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_billionaires(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    \n",
    "    billionaires_list = []\n",
    "    \n",
    "    \n",
    "    rows = soup.find_all('tr', class_='data')\n",
    "    \n",
    "    for row in rows:\n",
    "        details = {}\n",
    "        try:\n",
    "            details['Rank'] = row.find('td', class_='rank').text.strip()\n",
    "        except AttributeError:\n",
    "            details['Rank'] = \"-\"\n",
    "        \n",
    "        try:\n",
    "            details['Name'] = row.find('td', class_='name').text.strip()\n",
    "        except AttributeError:\n",
    "            details['Name'] = \"-\"\n",
    "        \n",
    "        try:\n",
    "            details['Net worth'] = row.find('td', class_='netWorth').text.strip()\n",
    "        except AttributeError:\n",
    "            details['Net worth'] = \"-\"\n",
    "        \n",
    "        try:\n",
    "            details['Age'] = row.find('td', class_='age').text.strip()\n",
    "        except AttributeError:\n",
    "            details['Age'] = \"-\"\n",
    "        \n",
    "        try:\n",
    "            details['Citizenship'] = row.find('td', class_='countryOfCitizenship').text.strip()\n",
    "        except AttributeError:\n",
    "            details['Citizenship'] = \"-\"\n",
    "        \n",
    "        try:\n",
    "            details['Source'] = row.find('td', class_='source').text.strip()\n",
    "        except AttributeError:\n",
    "            details['Source'] = \"-\"\n",
    "        \n",
    "        try:\n",
    "            details['Industry'] = row.find('td', class_='category').text.strip()\n",
    "        except AttributeError:\n",
    "            details['Industry'] = \"-\"\n",
    "        \n",
    "        billionaires_list.append(details)\n",
    "\n",
    "    return billionaires_list\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.forbes.com/billionaires/'  \n",
    "    billionaires = get_billionaires(url)\n",
    "    save_to_csv(billionaires, \"forbes_billionaires.csv\")\n",
    "    print(\"Data has been saved to forbes_billionaires.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae9db10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtube-comment-downloader\n",
      "  Downloading youtube_comment_downloader-0.1.76-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (from youtube-comment-downloader) (2.26.0)\n",
      "Collecting dateparser\n",
      "  Downloading dateparser-1.2.0-py2.py3-none-any.whl (294 kB)\n",
      "Requirement already satisfied: pytz in c:\\users\\admin\\anaconda3\\lib\\site-packages (from dateparser->youtube-comment-downloader) (2021.3)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from dateparser->youtube-comment-downloader) (2021.8.3)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\admin\\anaconda3\\lib\\site-packages (from dateparser->youtube-comment-downloader) (2.8.2)\n",
      "Collecting tzlocal\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil->dateparser->youtube-comment-downloader) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->youtube-comment-downloader) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->youtube-comment-downloader) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->youtube-comment-downloader) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests->youtube-comment-downloader) (1.26.7)\n",
      "Collecting tzdata\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Installing collected packages: tzdata, tzlocal, dateparser, youtube-comment-downloader\n",
      "Successfully installed dateparser-1.2.0 tzdata-2024.1 tzlocal-5.2 youtube-comment-downloader-0.1.76\n"
     ]
    }
   ],
   "source": [
    "# Q.8.:-\n",
    "!pip install youtube-comment-downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_comment_downloader import YoutubeCommentDownloader\n",
    "import pandas as pd\n",
    "\n",
    "def get_youtube_comments(video_url, max_comments=500):\n",
    "    downloader = YoutubeCommentDownloader()\n",
    "    comments = []\n",
    "    count = 0\n",
    "\n",
    "    for comment in downloader.get_comments_from_url(video_url):\n",
    "        if count >= max_comments:\n",
    "            break\n",
    "        comment_details = {\n",
    "            'Comment': comment['text'],\n",
    "            'Upvotes': comment['votes'],\n",
    "            'Posted Time': comment['time']\n",
    "        }\n",
    "        comments.append(comment_details)\n",
    "        count += 1\n",
    "\n",
    "    return comments\n",
    "\n",
    "def save_to_csv(comments, filename):\n",
    "    df = pd.DataFrame(comments)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_url = input(\"Enter the YouTube video URL: \")\n",
    "    comments = get_youtube_comments(video_url)\n",
    "    save_to_csv(comments, \"youtube_comments.csv\")\n",
    "    print(\"Data has been saved to youtube_comments.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b956f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.9.Answer:-\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_hostel_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    hostels_list = []\n",
    "    \n",
    "    hostel_cards = soup.find_all('div', class_='property-card')\n",
    "    \n",
    "    for hostel in hostel_cards:\n",
    "        hostel_details = {}\n",
    "        \n",
    "        # Hostel name\n",
    "        try:\n",
    "            hostel_details['Hostel Name'] = hostel.find('h2', class_='title').text.strip()\n",
    "        except AttributeError:\n",
    "            hostel_details['Hostel Name'] = \"-\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            hostel_details['Distance from City Centre'] = hostel.find('span', class_='distance-description').text.strip()\n",
    "        except AttributeError:\n",
    "            hostel_details['Distance from City Centre'] = \"-\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            hostel_details['Ratings'] = hostel.find('div', class_='score orange big').text.strip()\n",
    "        except AttributeError:\n",
    "            hostel_details['Ratings'] = \"-\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            hostel_details['Total Reviews'] = hostel.find('div', class_='reviews').text.strip().split()[0]\n",
    "        except AttributeError:\n",
    "            hostel_details['Total Reviews'] = \"-\"\n",
    "        \n",
    "        # Overall reviews\n",
    "        try:\n",
    "            hostel_details['Overall Reviews'] = hostel.find('div', class_='keyword').text.strip()\n",
    "        except AttributeError:\n",
    "            hostel_details['Overall Reviews'] = \"-\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            privates = hostel.find('a', class_='prices').find_all('div', class_='price')\n",
    "            hostel_details['Privates from Price'] = privates[0].text.strip() if privates else \"-\"\n",
    "        except AttributeError:\n",
    "            hostel_details['Privates from Price'] = \"-\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            dorms = hostel.find('a', class_='prices').find_all('div', class_='price')\n",
    "            hostel_details['Dorms from Price'] = dorms[1].text.strip() if len(dorms) > 1 else \"-\"\n",
    "        except AttributeError:\n",
    "            hostel_details['Dorms from Price'] = \"-\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            facilities = hostel.find('div', class_='facilities').find_all('ul')\n",
    "            facilities_list = [facility.text.strip() for facility in facilities]\n",
    "            hostel_details['Facilities'] = ', '.join(facilities_list)\n",
    "        except AttributeError:\n",
    "            hostel_details['Facilities'] = \"-\"\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            hostel_details['Property Description'] = hostel.find('div', class_='property-description').text.strip()\n",
    "        except AttributeError:\n",
    "            hostel_details['Property Description'] = \"-\"\n",
    "        \n",
    "        hostels_list.append(hostel_details)\n",
    "    \n",
    "    return hostels_list\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = 'https://www.hostelworld.com/findabed.php/ChosenCity.London/ChosenCountry.England' \n",
    "    hostels_data = get_hostel_data(url)\n",
    "    save_to_csv(hostels_data, \"hostels_in_london.csv\")\n",
    "    print(\"Data has been saved to hostels_in_london.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
